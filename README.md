# Cheap Experiments for Leverage University ğŸ”§ğŸŒ

> Â«ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° â€” ÑÑ‚Ğ¾ Ñ‚Ğ° Ñ‡Ğ°ÑÑ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸, Ğ³Ğ´Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµÑˆĞµĞ²Ñ‹.Â»  
> â€” Ğ’.Ğ˜. ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´

N.B. LaTeX rendering is trash on Github, vs code works great.

- [Cheap Experiments for Leverage University ğŸ”§ğŸŒ](#cheap-experiments-for-leverage-university-)
  - [Prospectus](#prospectus)
    - [Career Goals](#career-goals)
    - [Personal Motivations](#personal-motivations)
    - [Background](#background)
    - [Philosophy](#philosophy)
    - [Methodology \& Tools](#methodology--tools)
  - [Syllabus](#syllabus)
    - [Progress Tracking](#progress-tracking)
  - [Math Foundations â€” Two-Core Plan](#math-foundations--two-core-plan)
    - [Common base](#common-base)
    - [Goto Core (primary goal)](#goto-core-primary-goal)
    - [Modeling Core (secondary goal)](#modeling-core-secondary-goal)
  - [CS Core](#cs-core)
  - [Papers and Documentation](#papers-and-documentation)
    - [Philosophy \& Foundations](#philosophy--foundations)
    - [Performance Models](#performance-models)
    - [High-Performance Kernels](#high-performance-kernels)
    - [Memory \& Cache Models](#memory--cache-models)
    - [Compiler \& Program Transformations](#compiler--program-transformations)
    - [Machine Learning \& Differentiable Programming](#machine-learning--differentiable-programming)
  - [Domain Specific for Current Work](#domain-specific-for-current-work)
  - [Detours, Stretch Goals, Domain Specific](#detours-stretch-goals-domain-specific)


## Prospectus

### Career Goals

- Be the second coming of Kazushige Goto-san ğŸ™‡ğŸ¥·
- HPC ğŸï¸ğŸ’¨
- Modeling ğŸš‚
- Scientific Computing ğŸ§‘â€ğŸ”¬
- Compiler work ğŸ› ï¸

**North Star:** Ronin Perf LLC. âš”ï¸ğŸ¥·

### Personal Motivations

- Conquer the Math Dragon ğŸ§®ğŸ‰: persistent nerves about doing higher level math need to be confronted with exposure therapy. Don't be fooled, math is half intuition cultivation and half a bag of tricks ğŸª„. It is learnable.
- Find Leverage: the longer I ignore the closed form solutions lurking in math the more pathetic my programming gets. Î´á¿¶Ï‚ Î¼Î¿Î¹ Ï€á¾¶ ÏƒÏ„á¿¶ ÎºÎ±á½¶ Ï„á½°Î½ Î³á¾¶Î½ ÎºÎ¹Î½Î¬ÏƒÏ‰ ğŸŒğŸ”§.
- Don't Get Bored ğŸ¥±: it's easy to get the gist of every subject halfway through a book but math always goes deeper. It won't be boring.

### Background

I'm a software developer with minimal experience in math (some linear algebra and calculus). Now I want to make a career step change.

<!--  This section is commented out just for LLMs
### Status

36M. Currently a successful developer working as a quasi-build and test engineer (CI/CD, testing, maintaining build systems, and writing auxiliary programs) for a midsize semiconductor metrology company. Kind of a build and test engineer. More of a software mechanic with quick time to technical solution than a software developer with deep performance experience, but probably more knowledge about performance than the average developer. That is to say, I understand time complexity and op counting and what instruction pipelining is and memory latency, but haven't exactly spent time on avx intrinsics or assembly. Expertise in Python, experience in C, C++, C#, Rust, Java, and Javascript. Decent breadth of paradigm experience. 

No formal STEM background, more experience in literature, poetry, and classics. Majored in philosophy and classics with some specialization in logic and language:

- Phi of Lang
- Phi of Sci
- Phi of Mind
- Critical Thinking
- Symbolic Logic
- Modern Logic
- Aristotle's Logic
- Ancient Greek
- Latin

In my twenties I dipped into:

1. A random linear algebera textbook I never finished
2. Coding the Matrix (a linear algebra textbook) I never finished
3. A Programmer's Introduction to Mathematics (which covers linear algebra like SVD) but I never finished
4. A random calculus book I never finished (I got bored during Integration)

But I'm tired of CI/CD. I want to do the real under the hood work. I also find higher math interesting and elegant despite my historical aversion to math in general.
-->

### Philosophy

This isn't ultralearning in the traditional sense, I'm not speedrunning an MIT CS degree, I'm trying to surpass it. Building slow and deep and foundational knowledge and intuition. 

I am seeking a basis for the goal space: linearly independent and yet still spans the goal space.

I seek mathematical rigor to conquer the math dragon ğŸ‰. No half assed computationally focussed "this is how you do a matrix multiply" books.

This booklist is meant to reflect the actual best and most rigorous texts that form the basis. MIT will have stuff like Strang, but I choose Axler. I choose Spivak for the disguised real analysis rather than a cookbook of derivatives. Soviet math is of particular interest for its reputation for blending intuition and rigor. Soviet books get âš’ï¸. 

No bridge books ğŸŒ‰ğŸ“š! LLMs solve the difficulty cold start wall problem. Sometimes I'll dip into supplemental works.

I am also interested in domain specific goals that can add dimensionality and give a real leg up in a particularly interesting domain (Durbin's BSA, Arnold's CM, Hecht etc.).

This syllabus is STRICTLY about the math and text books not about manuals and hands on work. That is all covered in parallel documents.

### Methodology & Tools

Interleave 1-3 books at a time.

Anki helps to retain familiarity with definitions and challenging proofs encountered in work.

A Leuchtturm 1916 notebook contains a "Commonplace Book" of encountered proof tricks like "Completing the Square." The book will be refined with experience.

Work through the problems in a paper pad and nice notebook and also write them up in LaTeX. Solve the problems independently multiple times.

This process will take years. But every book completed should add to performance abilities at work. There won't be one day after completing this list where "Now you can work on HPC! ğŸ§‘â€ğŸ“." Likely the list will be refined as the books are completed and you identify paths yourself rather than with help.

---

## Syllabus

### Progress Tracking

- Axler 01: 1 month âœ”ï¸
- Axler 02: 1 month âœ”ï¸
- Spivak 01: Started July 28, problem 18/25 on August 12
- Spivak 02: 8/13/2025
- Axler 03a: 8/14/2025

Certainly this is a slow ramp up and will accelerate.

- ğŸ›ï¸: Still to buy
- ğŸŒ: Available Online
- ğŸ“–: Currently reading

## Math Foundations â€” Two-Core Plan

### Common base

1. Axler â€” Linear Algebra Done Right
   - Supplement: Shilov
   - Supplement: Gelfandâ€™s Lectures on Linear Algebra
2. Spivak â€” Calculus
   - Supplement: Schaumâ€™s Outline (exercises)
3. Spivak â€” Calculus on Manifolds

### Goto Core (primary goal)

1. Knuth et al. â€” Concrete Mathematics
2. Trefethen & Bau â€” Numerical Linear Algebra
   - Supplement: Golub & Van Loan â€” Matrix Computations
3. Higham â€” Accuracy and Stability of Numerical Algorithms
4. Saad â€” Iterative Methods for Sparse Linear Systems
5. ğŸ›ï¸ Boyd & Vandenberghe â€” Convex Optimization
6. Nocedal & Wright â€” Numerical Optimization
7. ğŸ›ï¸ Rivlin â€” Chebyshev Polynomials

### Modeling Core (secondary goal)

1. Arnold â€” Ordinary Differential Equations
2. ğŸ›ï¸ Durrett â€” Probability: Theory and Examples
3. Kolmogorov & Fomin â€” Elements of the Theory of Functions and Functional Analysis
   - Supplement: Kreyszig
4. ğŸ›ï¸ Samarskii â€” Theory of Difference Schemes
5. Tikhonov & Samarskii â€” Equations of Mathematical Physics
6. ğŸ›ï¸ KÃ¶rner â€” Fourier Analysis
7. ğŸ›ï¸ Ahlfors â€” Complex Analysis
8. ğŸ›ï¸ Trefethen â€” Spectral Methods in MATLAB

---

## CS Core

1. Bryant & O'Hallaron CSAPP
2. Sipser
3. CLRS â€“ Introduction to Algorithms
4. Engineering a Compiler [Alternative:Modern Compiler Design by Grune et al]
  - Supplement: ğŸ›ï¸ Muchnick's Advanced Compiler Design and Implementation
5. ğŸ›ï¸ Pierce â€“ Types and Programming Languages
6. ğŸ›ï¸ Hennessy & Patterson â€“ Computer Architecture: A Quantitative Approach
7. ğŸ›ï¸ Herlihy & Shavit â€“ Art of Multiprocessor Programming
8. ğŸ›ï¸ Hwu & Kirk â€“ Programming Massively Parallel Processors
9. ğŸ›ï¸ Gropp, Lusk & Skjellum â€“ Using MPI
10. ğŸ›ï¸ Hastie & Tibshirani â€“ Elements of Statistical Learning
11. ğŸ›ï¸ Warren â€“ Hacker's Delight
12. ğŸ›ï¸ OSTEP

---

## Papers and Documentation

### Philosophy & Foundations

- Thurston â€” â€œOn Proof and Progress in Mathematicsâ€: philosophy of mathematics, discovery, and pedagogy.
- Wigner â€” â€œThe Unreasonable Effectiveness of Mathematics in the Natural Sciencesâ€: reflections on the mathematicsâ€“physics relationship.

### Performance Models

- Williams, Waterman & Patterson â€” â€œRoofline: An Insightful Visual Performance Model for Multicore Architecturesâ€: performance modeling of memory- vs compute-bound workloads.
- Hong & Kung â€” â€œI/O Complexity: The Redâ€“Blue Pebble Gameâ€ (1981): foundational lower bounds on communication complexity.
- Culler et al. â€” â€œLogP: Towards a Realistic Model of Parallel Computationâ€ (1993): classic model for distributed-memory performance.
- Gustafson â€” â€œReevaluating Amdahlâ€™s Lawâ€ (1988): scalability model (weak scaling vs Amdahlâ€™s pessimism).

### High-Performance Kernels

- Goto & van de Geijn â€” â€œAnatomy of High-Performance Matrix Multiplicationâ€: cache optimization, BLAS kernel design.
- Whaley & Dongarra â€” â€œAutomatically Tuned Linear Algebra Software (ATLAS)â€: systematic search-based kernel optimization.
- Yotov et al. â€” â€œIs Search Really Necessary to Generate High-Performance BLAS?â€: prediction vs empirical tuning.
- Frigo & Johnson â€” â€œThe Design and Implementation of FFTW (â€˜The Fastest Fourier Transform in the Westâ€™)â€: auto-tuning in FFT kernels.
- Volkov & Demmel â€” â€œLU, QR, and Cholesky Factorizations Using Vector Capabilities of GPUsâ€ (2008): GPU kernel optimization, a Goto-style paper for accelerators.

### Memory & Cache Models

- Lam, Wolf & Lam â€” â€œThe Cache Performance and Optimizations of Blocked Algorithmsâ€: cache blocking theory.
- Frigo, Leiserson, Prokop & Ramachandran â€” â€œCache-Oblivious Algorithmsâ€: recursive cache-friendly algorithms without tuning parameters.

### Compiler & Program Transformations

- Lattner & Adve â€” â€œLLVM: A Compilation Framework for Lifelong Program Analysis & Transformationâ€: modular compiler infrastructure.
- Bastoul â€” â€œCode Generation in the Polyhedral Modelâ€: loop transformations and dependence analysis.
- Allen & Kennedy â€” â€œOptimizing Compilers for Modern Architecturesâ€ (overview paper): classic introduction to compiler optimizations, ties into Muchnick.

### Machine Learning & Differentiable Programming

- Baydin et al. â€” â€œAutomatic Differentiation in Machine Learning: A Surveyâ€: AD foundations and systems.
- Vaswani et al. â€” â€œAttention Is All You Needâ€: the Transformer architecture, parallelism-friendly ML.
- Dean & Ghemawat â€” â€œMapReduce: Simplified Data Processing on Large Clustersâ€: distributed data processing at scale.


## Domain Specific for Current Work

These might help with my current job directly.

- Streetman & Banerjee â€“ Solid State Electronic Devices
- Arnold's Mathematical Methods of Classical Mechanics âš’ï¸
- ğŸ›ï¸ Hecht's Optics
- ğŸ›ï¸ Griffiths â€“ Introduction to Electrodynamics

## Detours, Stretch Goals, Domain Specific 

It is fine if these seem completely ancillary or redundant. This is a subscription list to expand and pick through after the main curriculum.

- Durbin's Biological Sequence Analysis
- ğŸ›ï¸ Cover & Joy â€“ Elements of Information Theory
- ğŸ›ï¸ Nielsen & Chuang â€“ Quantum Computation and Quantum Information
- ğŸ›ï¸ Shreve â€“ Stochastic Calculus for Finance II
- ğŸ›ï¸ Anderson â€“ Computational Fluid Dynamics: The Basics with Applications: Navier-Stokes is the perfect PDE playground for HPC. Ties together everything: PDEs, numerical methods, parallel computing. Plus, fluid flow shows up in semiconductor manufacturing.
- ğŸ›ï¸ Pharr, Jakob & Humphreys â€“ Physically Based Rendering (PBRT): Combines Monte Carlo methods (Durrett), optics (Hecht), and serious optimization. Rendering is embarrassingly parallel - perfect HPC domain. Ray tracing for metrology simulation.
- ğŸ›ï¸ Diestel â€“ Graph Theory